# Task ID: 3
# Title: LLM Integration Module (Groq API + Ollama)
# Status: done
# Dependencies: 2
# Priority: high
# Description: Create the core module for integrating with multiple LLM providers including Groq API (primary) and Ollama (fallback)
# Details:
Implement the GroqClient class for fast cloud inference and OllamaClient class for local LLM service. Include dual provider support with environment-based selection, OS-aware prompts for cross-platform command generation, and multiple model support (Llama3-8B, Mixtral-8x7B, Gemma). Handle connection failures, model validation, and timeout scenarios for both providers.

# Test Strategy:
Test connection to Groq API and local Ollama instance, verify model loading and switching, test basic inference with sample prompts, and validate cross-platform command generation.
